# Fichamento

Buscar informações em textos através de Natural Language Processing (NLP) utilizando Named Entity Recognition (NER).

“It corresponds to the recognition and categorization of entities mentioned in a text sample or corpus. Examples of named entities are proper names, events, places, temporal and quantitative data, etc. These extracted entities can then be further mapped to a knowledge base (KB), in special a Digital Humanities KB (DHKB) [2]. These DHKBs allow for the identification and combination of existing knowledge about historical facts from different sources.” (Zilio et al., 2022, p. 2)

O que é Digital Humanities knowledge base? Ver:

“Golub, K., Liu, Y.H.: Information and knowledge organisation in digital humanities: Global perspectives (2022)” (Zilio et al., 2022, p. 9)

## Objetivo do artigo:

Identificar um sistema de NER para o português que funcione para extração de informação de textos médicos do século XVIII.

## Método (p.2)

1. Criação de um “gold standard” baseado em um transcição modernizada de três amostras de textos; e em seguida avaliaram o sistema de extração de entidades nomeadas contra esse “gold standard”;
2. Comparação enter o sistema de extração de NE nas transcrições modernizadas com a extração de NE nas transcrições “não-modernizadas”;
3. Depois de avaliar o melhor sistema de NER baeado nesses dois experimentos, o objetivo foi conduzir uma extração completa de entidades nomeadas do *corpus* médico do século XVIII.

## Related work

A questão do OCR:

“The authors also report that most of the errors occurred because of the low quality of the OCR result.” (Zilio et al., 2022, p. 2)

A questão do treinamento:

“For medieval French, Aguilar and Stutzmann [1] present a corpus and trained a new system for legal documents of the XIII and XIV centuries. Their performance measures are around 90%.” (Zilio et al., 2022, p. 3)

## Methodology

### 3.1 Corpus

Três amostras de textos variados no mesmo período, com extensão similar. Utilizaram versão modernizadas e não-modernizadas para o estudo.

### 3.2 Gold standard

Criado a partir das versões modernizadas.

“Two linguists, authors of this paper, annotated all samples independently and exhaustively.” (Zilio et al., 2022, p. 4)

Total de 262 entidades nomeadas.

[zilo1.png]  
(Zilio et al., 2022, p. 5)

### 3.3 Off-the-shelf NER tools

Três modelos: 2 do spaCy (pretreinados: spaCy lg and spaCy sm) e um é um modelo BERT-CRF treinado para o português (Souza et al.)

### 3.4 Evaluation approach

“we analyzed whether the systems were able to extract the correct NEs from the samples, without taking the source segments into consideration.” (Zilio et al., 2022, p. 6)

## Results (p. 6)

### 4.1. NER on modernized samples

[zilo2.png]  
(Zilio et al., 2022, p. 6)

“Looking at the annotations generated by each system, it becomes clear that BERT-CRF had a better precision for the annotation of NE tags.” (Zilio et al., 2022, p. 6)

“Some recurrent mistakes of this system were the annotation of locations as persons and also the annotation of any digit as a value.” (Zilio et al., 2022, p. 6)

spaCy:

“incorrect annotation of tokens as NE, and also in the annotation of persons as locations (the opposite of BERT-CRF model).” (Zilio et al., 2022, p. 7)

“the tagging of extra tokens preceding or following a NE.” (Zilio et al., 2022, p. 7)

“This was not a problem at all for BERT-CRF.” (Zilio et al., 2022, p. 7)

“BERT-CRF had the best result when analyzing the extraction from a modernized version of the texts. However, since the process of modernizing these texts is similar to the process of a translation, it is unrealistic to expect all historical texts to be translated before applying a NER system to them. So we cannot use an NE extraction based on modernized versions as a parameter for old texts.” (Zilio et al., 2022, p. 7)

### 4.2 NER systems: modernized vs. non-modernized extraction

[zilo3.png]  
(Zilio et al., 2022, p. 7)

### 4.3 Annotation of Semedo’s Work

“The non-standard spelling”:

“even in such adverse context, BERT-CRF annotations were still consistent, and it proved to be the most robust of the three models, as it was able to handle one of the main issues of working with old texts: the non-standard spelling.” (Zilio et al., 2022, p. 8)

A anotação está disponível aqui: [https://github.com/uebelsetzer/NER_for_Portuguese_XVIII-Century_Texts](https://github.com/uebelsetzer/NER_for_Portuguese_XVIII-Century_Texts)

## Final Remarks

“After analyzing the results of both experiments, we concluded that BERTCRF had better performance, even when considering the original spelling of the historical texts. Both spaCy models had issues in recognizing NEs, changing the tag of many entities and adding wrong NEs, especially in the non-modernized versions of the texts. Considering these results, we used BERT-CRF to annotate a large sample of non-modernized texts extracted from Semedo’s work Observa ̧coens medicas doutrinaes de cem casos gravissimos.” (Zilio et al., 2022, p. 9)
