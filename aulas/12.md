---
title: "Ollama na Pr√°tica: Modelos Locais de Linguagem"
subtitle: "<br>Oficina V"
date: today 
date-format: full
lang: pt-br
format:
  revealjs: 
    theme: [default, custom.scss]
    slide-number: true
    incremental: false
    chalkboard: 
      buttons: true
    footer: "Eric Brasil | <a href=https://ericbrasil.com.br/contact/>Entre em contato</a> | IM1256 - Introdu√ß√£o √† Hist√≥ria Digital (PPGIHD/UFRRJ)"
    logo: https://omekas.im.ufrrj.br/files/original/aa99fe174fd6f97dd42ee78a359a46428b9997be.png
author:
  - name: Eric Brasil
    email: profericbrasil@gmail.com
    orcid: 0000-0001-5067-8475
    affiliation: UNILAB e PPGIHD/UFRRJ
description: "Oficina pr√°tica da disciplina IM1256 - Introdu√ß√£o √† Hist√≥ria Digital, ministrada pelo Prof. Eric Brasil no PPGIHD/UFRRJ. Esta oficina apresenta fundamentos e aplica√ß√µes do Ollama e de modelos locais de linguagem para pesquisa e experimenta√ß√£o com IA generativa."
toc: false
---

## ü§î O que √© o Ollama?

- Plataforma para **rodar Modelos de Linguagem (LLMs)** localmente  
- Permite executar **modelos abertos** (LLaMA, Mistral, Gemma, Phi etc.) em computadores pessoais  
- Cria um **servidor local** acess√≠vel via terminal, API ou interfaces como o **Open WebUI**

üìç **Objetivo**: democratizar o acesso a modelos de IA sem depender de nuvem ou servi√ßos pagos.

---

## ‚öôÔ∏è Como o Ollama funciona

- Cada modelo √© armazenado como um **container local (modelfile)**  
- Pode ser executado e chamado via **CLI** ou **API REST**  
- Baseado em **llama.cpp**, otimizado para CPUs e GPUs de uso comum  

Exemplo:
```bash
ollama run mistral
````

Na primeira execu√ß√£o, o modelo √© baixado automaticamente.

---

## üì¶ Principais Modelos Dispon√≠veis {.center}

[Clique aqui para ver a lista completa](https://ollama.com/library)

---

## üíª Vantagens do Ollama {.center}

- Execu√ß√£o **offline**, sem enviar dados a servidores externos
- Maior **privacidade e reprodutibilidade**
- Integra√ß√£o com **Python, Node e REST APIs**
- Suporte √† **quantiza√ß√£o**, reduzindo RAM e VRAM necess√°rias
- Compat√≠vel com **Open WebUI**, **LM Studio**, entre outros

---

## üöÄ Instala√ß√£o e primeiros comandos {.center}

```bash
# Instalar (Linux)
curl -fsSL https://ollama.com/install.sh | sh

# Ver modelos dispon√≠veis
ollama list

# Baixar um modelo
ollama pull phi3

# Rodar e conversar
ollama run phi3
```

---

## üß† Pr√°tica: rodando modelos pequenos {.center}

Vamos testar juntos:

1. **Baixe o modelo Phi-3-mini (3.8B)**

   ```bash
   ollama pull phi3:mini
   ```

2. **Teste com um prompt**

   ```bash
   ollama run phi3:mini "Explique o conceito de Humanidades Digitais em 3 linhas"
   ```

---

## {.center}

3. **Use via API (Python)**

   ```python
   import requests

   r = requests.post("http://localhost:11434/api/generate",
       json={"model": "phi3:mini",
             "prompt": "Explique RAG em 3 linhas"})
   print(r.json()["response"])
   ```

---

## üîí √âtica e sustentabilidade {.center}

* Execu√ß√£o local ‚Üí **menos depend√™ncia de servidores externos**
* **Privacidade garantida** (dados n√£o enviados √† nuvem)
* Contribui para a **soberania tecnol√≥gica** e a **autonomia digital**

---

## üéØ Atividade pr√°tica {.center}

üí° **Tarefas:**

1. Rodar **dois modelos pequenos** (Phi, Qwen ou Mistral) via GUI do Ollama ou terminal.
2. Criar um **script em Python** que gere receba um documento de texto + um prompt e retorne uma resposta sobre o conte√∫do do documento.

---

## ü§ù Cr√©ditos {.center}

üõ†Ô∏è Slides e formata√ß√£o criados com apoio do ChatGPT (modelo **GPT-5**) entre os dias **10 e 11 de novembro de 2025**, sob orienta√ß√£o editorial e pedag√≥gica de **Eric Brasil**.
