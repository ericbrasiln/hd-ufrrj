{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f1e002",
   "metadata": {},
   "source": [
    "# Processamento de linguagem natural com Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d92fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalando as bibliotecas necessárias\n",
    "!spacy\n",
    "!pandas\n",
    "!ipython\n",
    "!matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183d8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando a biblioteca spaCy\n",
    "import spacy\n",
    "from spacy.cli import download\n",
    "from spacy import displacy\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac8777d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baixando o modelo de linguagem em português\n",
    "#download(\"pt_core_news_sm\")\n",
    "download(\"pt_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e94a80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# carregando o modelo de linguagem em português\n",
    "nlp = spacy.load(\"pt_core_news_lg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02369b66",
   "metadata": {},
   "source": [
    "## Preparando o documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c8511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lendo o arquivo de texto\n",
    "texto = \"./helena.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762b2a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# abrindo o arquivo e lendo o conteúdo\n",
    "with open(texto, \"r\", encoding=\"utf-8\") as file:\n",
    "    texto = file.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9b093c",
   "metadata": {},
   "source": [
    "### Limpeza simples do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1955124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#limpar o texto (remover espaços extras, quebras de linha desnecessárias, colocar em minusculas, remover pontuações)\n",
    "texto_limpo = re.sub(r'\\s+', ' ', texto)  # remover quebras de linha e espaços extras\n",
    "texto_limpo = texto_limpo.strip()  # remover espaços no início e no fim\n",
    "texto_limpo = texto_limpo.lower()  # converter para minúsculas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f38493c",
   "metadata": {},
   "source": [
    "## Processar o texto com o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd83372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processando o texto com o modelo carregado\n",
    "doc = nlp(texto_limpo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9d5305",
   "metadata": {},
   "source": [
    "### Token\n",
    "\n",
    "O token é a unidade básica de texto em processamento de linguagem natural (PLN). Pode ser uma palavra, um número, um símbolo ou até mesmo um espaço em branco. A tokenização é o processo de dividir um texto em tokens, facilitando a análise e o processamento do conteúdo textual.\n",
    "\n",
    "### Partes do Discurso (POS - Part of Speech)\n",
    "\n",
    "As partes do discurso (POS) são categorias gramaticais que descrevem a função de uma palavra em uma frase. Exemplos comuns de partes do discurso incluem substantivos, verbos, adjetivos, advérbios, pronomes, preposições, conjunções e interjeições. A identificação correta das partes do discurso é essencial para entender o significado e a estrutura das frases.\n",
    "\n",
    "### Dependências\n",
    "\n",
    "As dependências gramaticais descrevem as relações entre as palavras em uma frase. Elas indicam como as palavras estão conectadas umas às outras, mostrando quais palavras dependem de outras para formar um significado completo. Por exemplo, em uma frase como \"O gato está no telhado\", a palavra \"gato\" depende do verbo \"está\" para completar o sentido da frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8445121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# listar primeiras 10 tokens do texto com suas respectivas partes do discurso e dependências\n",
    "print(\"Tokens do texto (de 1000 a 1040), com POS e DEP:\")\n",
    "for token in doc[1000:1040]: # iterando sobre os tokens do texto\n",
    "    print(token.text, token.pos_, token.dep_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573d5d77",
   "metadata": {},
   "source": [
    "### Tabela de POS\n",
    "\n",
    "| Abreviação | Parte do Discurso       | Descrição                                      |\n",
    "|------------|------------------------|------------------------------------------------|\n",
    "| NOUN       | Substantivo            | Nome de pessoas, lugares, coisas ou ideias     |\n",
    "| VERB       | Verbo                  | Ação ou estado de ser                           |\n",
    "| ADP        | Adposição              | Inclui preposições e pós-posições               |\n",
    "| ADJ        | Adjetivo               | Descreve ou modifica um substantivo               |\n",
    "| ADV        | Advérbio               | Modifica um verbo, adjetivo ou outro advérbio     |\n",
    "| PRON       | Pronome                | Substitui um substantivo                          |\n",
    "| PREP       | Preposição             | Indica relações entre palavras                   |\n",
    "| CONJ       | Conjunção              | Conecta palavras, frases ou orações          |\n",
    "| INTJ       | Interjeição            | Expressa emoção ou reação                        |\n",
    "| DET        | Determinante           | Especifica um substantivo                         |\n",
    "| AUX        | Verbo Auxiliar         | Ajuda a formar tempos verbais, modos ou vozes    |\n",
    "| PUNCT      | Pontuação              | Símbolos de pontuação                             |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed7f3f7",
   "metadata": {},
   "source": [
    "### Tabela de dependências\n",
    "\n",
    "| Abreviação | Descrição                                      |\n",
    "|------------|------------------------------------------------|\n",
    "| nsubj      | Sujeito nominal                                |\n",
    "| obj        | Objeto direto                                  |\n",
    "| iobj       | Objeto indireto                                |\n",
    "| amod       | Modificador adjetival                          |\n",
    "| advmod     | Modificador adverbial                          |\n",
    "| nmod       | Modificador nominal                            |\n",
    "| det        | Determinante                                   |\n",
    "| case       | Palavra de caso (preposição)                   |\n",
    "| root       | Raiz da frase                                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d59353c",
   "metadata": {},
   "source": [
    "### Lemmatizar\n",
    "\n",
    "O processo de lematização consiste em reduzir uma palavra à sua forma base ou dicionário, conhecida como lema. Por exemplo, as palavras \"correr\", \"correndo\" e \"correu\" têm o mesmo lema \"correr\". A lematização é útil em várias aplicações de processamento de linguagem natural (PLN), como análise de sentimentos, recuperação de informações e tradução automática, pois ajuda a agrupar diferentes formas de uma palavra sob um único conceito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23ca2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmas dos 50 primeiros verbos\n",
    "print(\"\\nLemas dos primeiros 50 verbos:\")\n",
    "verb_count = 0 # contador de verbos encontrados\n",
    "for token in doc:\n",
    "    if token.pos_ == \"VERB\": # verificando se o token é um verbo\n",
    "        print(token.text, token.lemma_)\n",
    "        verb_count += 1\n",
    "    if verb_count >= 50: # se já encontrou 100 verbos, sai do loop\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e098530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contar os 10 verbos mais comuns no texto (após lematização)\n",
    "from collections import Counter # importando Counter para contagem\n",
    "verb_lemmas = [token.lemma_ for token in doc if token.pos_ == \"VERB\"] # lista de lemas dos verbos usando list comprehension\n",
    "verb_freq = Counter(verb_lemmas) # contando a frequência dos lemas dos verbos\n",
    "print(\"\\n10 verbos mais comuns no texto (após lematização):\")\n",
    "for verb, freq in verb_freq.most_common(10):\n",
    "    print(verb, freq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfe85b4",
   "metadata": {},
   "source": [
    "### NER\n",
    "\n",
    "A Reconhecimento de Entidades Nomeadas (NER - Named Entity Recognition) é uma técnica de processamento de linguagem natural (PLN) que identifica e classifica entidades mencionadas em um texto em categorias predefinidas, como pessoas, organizações, locais, datas, valores monetários, entre outras. O NER é amplamente utilizado em várias aplicações, como análise de sentimentos, extração de informações e sistemas de perguntas e respostas.\n",
    "\n",
    "#### Tabela de NER\n",
    "\n",
    "| Abreviação | Entidade               | Descrição                                      |\n",
    "|------------|-----------------------|------------------------------------------------|\n",
    "| PERSON     | Pessoa                | Nomes de pessoas, personagens ou indivíduos     |\n",
    "| ORG        | Organização           | Nomes de empresas, instituições ou grupos       |\n",
    "| GPE        | Localização           | Nomes de países, cidades ou estados             |\n",
    "| LOC        | Local                 | Nomes de locais geográficos, como montanhas ou rios |\n",
    "| DATE       | Data                  | Referências a datas ou períodos de tempo        |    \n",
    "| TIME       | Hora                  | Referências a horários ou períodos do dia       |\n",
    "| MISC       | Miscelânea            | Entidades que não se encaixam em outras categorias |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca6e04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# listar 10 Pessoas mais comuns no texto\n",
    "person_entities = [ent.text for ent in doc.ents if ent.label_ == \"PER\"] # lista de entidades do tipo Pessoa\n",
    "person_freq = Counter(person_entities) # contando a frequência das entidades Pessoa\n",
    "print(\"\\n10 Pessoas mais comuns no texto:\")\n",
    "for person, freq in person_freq.most_common(10):\n",
    "    print(person, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36e74a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# listar 10 localizações mais comuns no texto\n",
    "location_entities = [ent.text for ent in doc.ents if ent.label_ == \"LOC\"] # lista de entidades do tipo Localização\n",
    "location_freq = Counter(location_entities) # contando a frequência das entidades Localização\n",
    "print(\"\\n10 Localizações mais comuns no texto:\")\n",
    "for location, freq in location_freq.most_common(10):\n",
    "    print(location, freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95267978",
   "metadata": {},
   "source": [
    "#### Visualizar NER com displacy\n",
    "\n",
    "O displacy é uma ferramenta de visualização integrada ao spaCy que permite exibir entidades nomeadas (NER) e dependências gramaticais de forma interativa e visualmente atraente. Ele pode ser usado em notebooks Jupyter ou em páginas web para facilitar a análise e compreensão do texto processado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b972df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# usar o displacy para visualizar as entidades nomeadas\n",
    "html = displacy.render(doc, style=\"ent\", jupyter=False)\n",
    "#salvar em um arquivo HTML\n",
    "with open(\"entidades.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc700d9",
   "metadata": {},
   "source": [
    "## Análise semântica com vetores de palavras\n",
    "\n",
    "Com spacy é possível acessar os vetores de palavras (word vectors) que representam semanticamente as palavras em um espaço vetorial. Esses vetores capturam relações semânticas entre palavras, permitindo realizar operações como encontrar palavras semelhantes, calcular similaridade entre palavras e frases, e realizar tarefas de clustering e classificação baseadas em significado.\n",
    "\n",
    "Vamos testar com as palavras \"mulher\", \"homem\", \"escravo\", \"senhor\", \"negro\", \"branco\". O objetivo é verificar quais palavras no texto são semanticamente mais próximas dessas três palavras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dd1d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos explorar os vetores de palavras mulher e escravo\n",
    "word1 = nlp(\"mulher\")\n",
    "\n",
    "# encontrar as 10 palavras mais similares a \"mulher\"\n",
    "print(\"\\n10 palavras mais similares a 'mulher':\")\n",
    "similar_words = sorted(\n",
    "    doc.vocab,\n",
    "    key=lambda w: word1.similarity(nlp(w.text)), # calculando similaridade\n",
    "    reverse=True\n",
    "    ) # ordenando palavras por similaridade\n",
    "count = 0\n",
    "for w in similar_words:\n",
    "    if w.has_vector and w.is_lower and w.is_alpha:# garantindo que a palavra tem vetor, é minúscula e é alfabética\n",
    "        print(w.text, word1.similarity(nlp(w.text)))# imprimindo palavra e similaridade\n",
    "        count += 1\n",
    "    if count >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ed9d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agora para a palavra \"homem\"\n",
    "word2 = nlp(\"homem\")\n",
    "\n",
    "# encontrar as 10 palavras mais similares a \"homem\"\n",
    "print(\"\\n10 palavras mais similares a 'homem':\")\n",
    "similar_words = sorted(doc.vocab, key=lambda w: word2.similarity(nlp(w.text)), reverse=True)\n",
    "count = 0\n",
    "for w in similar_words:\n",
    "    if w.has_vector and w.is_lower and w.is_alpha:\n",
    "        print(w.text, word2.similarity(nlp(w.text)))\n",
    "        count += 1\n",
    "    if count >= 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f813e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"escravo\"\n",
    "word3 = nlp(\"escravo\")\n",
    "# encontrar as 10 palavras mais similares a \"escravo\"\n",
    "print(\"\\n10 palavras mais similares a 'escravo':\")\n",
    "similar_words = sorted(doc.vocab, key=lambda w: word3.similarity(nlp(w.text)), reverse=True)\n",
    "count = 0\n",
    "for w in similar_words:\n",
    "    if w.has_vector and w.is_lower and w.is_alpha:\n",
    "        print(w.text, word3.similarity(nlp(w.text)))\n",
    "        count += 1\n",
    "    if count >= 10:\n",
    "        break                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a01536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"senhor\"\n",
    "word4 = nlp(\"senhor\")\n",
    "# encontrar as 10 palavras mais similares a \"negro\"\n",
    "print(\"\\n10 palavras mais similares a 'senhor':\")\n",
    "similar_words = sorted(doc.vocab, key=lambda w: word4.similarity(nlp(w.text)), reverse=True)\n",
    "count = 0\n",
    "for w in similar_words:\n",
    "    if w.has_vector and w.is_lower and w.is_alpha:\n",
    "        print(w.text, word4.similarity(nlp(w.text)))\n",
    "        count += 1\n",
    "    if count >= 10:\n",
    "        break   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef51172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"negro\"\n",
    "word5 = nlp(\"negro\")\n",
    "# encontrar as 10 palavras mais similares a \"negro\"\n",
    "print(\"\\n10 palavras mais similares a 'negro':\")\n",
    "similar_words = sorted(doc.vocab, key=lambda w: word4.similarity(nlp(w.text)), reverse=True)\n",
    "count = 0\n",
    "for w in similar_words:\n",
    "    if w.has_vector and w.is_lower and w.is_alpha:\n",
    "        print(w.text, word5.similarity(nlp(w.text)))\n",
    "        count += 1\n",
    "    if count >= 10:\n",
    "        break                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d606848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"branco\"\n",
    "word6 = nlp(\"branco\")\n",
    "# encontrar as 10 palavras mais similares a \"branco\"\n",
    "print(\"\\n10 palavras mais similares a 'branco':\")\n",
    "similar_words = sorted(doc.vocab, key=lambda w: word5.similarity(nlp(w.text)), reverse=True)\n",
    "count = 0\n",
    "for w in similar_words:\n",
    "    if w.has_vector and w.is_lower and w.is_alpha:\n",
    "        print(w.text, word6.similarity(nlp(w.text)))\n",
    "        count += 1\n",
    "    if count >= 10:\n",
    "        break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05624389",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f656bb",
   "metadata": {},
   "source": [
    "## Visualização de similaridade semântica com gráfico 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28752ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_similar_from_vocab(target_vec, target_text, vocab, n=7):\n",
    "    \"\"\"Retorna as n palavras mais similares ao vetor alvo a partir do vocabulário fornecido.\"\"\"\n",
    "    candidates = []\n",
    "    tnorm = np.linalg.norm(target_vec)\n",
    "    for lex in vocab:\n",
    "        if not (lex.has_vector and lex.is_lower and lex.is_alpha):\n",
    "            continue\n",
    "        if lex.text == target_text:\n",
    "            continue\n",
    "        v = lex.vector\n",
    "        vn = np.linalg.norm(v)\n",
    "        if vn == 0 or tnorm == 0:\n",
    "            continue\n",
    "        sim = float(np.dot(target_vec, v) / (tnorm * vn))\n",
    "        candidates.append((lex.text, sim))\n",
    "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "    return candidates[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1634bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mulher (azul) e homem (verde)\n",
    "target1_text = \"mulher\"\n",
    "target2_text = \"homem\"\n",
    "t1 = nlp(target1_text)[0]\n",
    "t2 = nlp(target2_text)[0]\n",
    "\n",
    "if nlp.vocab.vectors_length == 0:\n",
    "    raise RuntimeError(\"O modelo atual não tem vetores (nlp.vocab.vectors_length == 0). \"\n",
    "                       \"Instale um modelo com vetores: python -m spacy download pt_core_news_md (ou lg).\")\n",
    "\n",
    "topn = 10\n",
    "top1 = [w for w,_ in top_n_similar_from_vocab(t1.vector, t1.text, nlp.vocab, n=topn)]\n",
    "top2 = [w for w,_ in top_n_similar_from_vocab(t2.vector, t2.text, nlp.vocab, n=topn)]\n",
    "\n",
    "# criar lista única preservando ordem (primeiro alvo1 e seus top, depois alvo2 e seus top não repetidos)\n",
    "words = [target1_text] + top1 + [w for w in [target2_text] + top2 if w not in ([target1_text] + top1)]\n",
    "\n",
    "# montar matriz de vetores\n",
    "vecs = [nlp.vocab[w].vector for w in words]\n",
    "X = np.vstack(vecs)\n",
    "\n",
    "# projetar em 2D com SVD\n",
    "Xc = X - X.mean(axis=0)\n",
    "U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
    "proj = Xc.dot(Vt[:2].T)\n",
    "\n",
    "# determinar cor por origem\n",
    "colors = []\n",
    "for w in words:\n",
    "    if w == target1_text:\n",
    "        colors.append('tab:blue')\n",
    "    elif w == target2_text:\n",
    "        colors.append('tab:green')\n",
    "    elif w in top1 and w in top2:\n",
    "        colors.append('tab:purple')  # aparece em ambos\n",
    "    elif w in top1:\n",
    "        colors.append('#4da6ff')  # azul claro\n",
    "    elif w in top2:\n",
    "        colors.append('#66cc66')  # verde claro\n",
    "    else:\n",
    "        colors.append('gray')\n",
    "\n",
    "origin = np.array([0.0, 0.0])\n",
    "plt.figure(figsize=(10,6))\n",
    "for i, w in enumerate(words):\n",
    "    x, y = proj[i]\n",
    "    width_head = 0.06 * max(1, np.linalg.norm(proj[i]))\n",
    "    length_head = 0.09 * max(1, np.linalg.norm(proj[i]))\n",
    "    linewidth = 2.2 if w in (target1_text, target2_text) else 1.2\n",
    "    plt.arrow(origin[0], origin[1], x, y,\n",
    "              head_width=width_head, head_length=length_head,\n",
    "              fc=colors[i], ec=colors[i], length_includes_head=True,\n",
    "              alpha=0.9, linewidth=linewidth)\n",
    "    plt.text(x*1.06, y*1.06, w, fontsize=12, ha='center', va='center', color=colors[i])\n",
    "\n",
    "plt.axhline(0, color='gray', linewidth=0.5)\n",
    "plt.axvline(0, color='gray', linewidth=0.5)\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "plt.title(f\"Vetores de palavras em 2D — '{target1_text}' (azul) e '{target2_text}' (verde)\")\n",
    "plt.xlabel(\"Dimensão 1\")\n",
    "plt.ylabel(\"Dimensão 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb091d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# branco (azul) e negro (verde)\n",
    "target1_text = \"branco\"\n",
    "target2_text = \"negro\"\n",
    "t1 = nlp(target1_text)[0]\n",
    "t2 = nlp(target2_text)[0]\n",
    "\n",
    "if nlp.vocab.vectors_length == 0:\n",
    "    raise RuntimeError(\"O modelo atual não tem vetores (nlp.vocab.vectors_length == 0). \"\n",
    "                       \"Instale um modelo com vetores: python -m spacy download pt_core_news_md (ou lg).\")\n",
    "\n",
    "topn = 10\n",
    "top1 = [w for w,_ in top_n_similar_from_vocab(t1.vector, t1.text, nlp.vocab, n=topn)]\n",
    "top2 = [w for w,_ in top_n_similar_from_vocab(t2.vector, t2.text, nlp.vocab, n=topn)]\n",
    "\n",
    "# criar lista única preservando ordem (primeiro alvo1 e seus top, depois alvo2 e seus top não repetidos)\n",
    "words = [target1_text] + top1 + [w for w in [target2_text] + top2 if w not in ([target1_text] + top1)]\n",
    "\n",
    "# montar matriz de vetores\n",
    "vecs = [nlp.vocab[w].vector for w in words]\n",
    "X = np.vstack(vecs)\n",
    "\n",
    "# projetar em 2D com SVD\n",
    "Xc = X - X.mean(axis=0)\n",
    "U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
    "proj = Xc.dot(Vt[:2].T)\n",
    "\n",
    "# determinar cor por origem\n",
    "colors = []\n",
    "for w in words:\n",
    "    if w == target1_text:\n",
    "        colors.append('tab:blue')\n",
    "    elif w == target2_text:\n",
    "        colors.append('tab:green')\n",
    "    elif w in top1 and w in top2:\n",
    "        colors.append('tab:purple')  # aparece em ambos\n",
    "    elif w in top1:\n",
    "        colors.append('#4da6ff')  # azul claro\n",
    "    elif w in top2:\n",
    "        colors.append('#66cc66')  # verde claro\n",
    "    else:\n",
    "        colors.append('gray')\n",
    "\n",
    "origin = np.array([0.0, 0.0])\n",
    "plt.figure(figsize=(10,6))\n",
    "for i, w in enumerate(words):\n",
    "    x, y = proj[i]\n",
    "    width_head = 0.06 * max(1, np.linalg.norm(proj[i]))\n",
    "    length_head = 0.09 * max(1, np.linalg.norm(proj[i]))\n",
    "    linewidth = 2.2 if w in (target1_text, target2_text) else 1.2\n",
    "    plt.arrow(origin[0], origin[1], x, y,\n",
    "              head_width=width_head, head_length=length_head,\n",
    "              fc=colors[i], ec=colors[i], length_includes_head=True,\n",
    "              alpha=0.9, linewidth=linewidth)\n",
    "    plt.text(x*1.06, y*1.06, w, fontsize=12, ha='center', va='center', color=colors[i])\n",
    "\n",
    "plt.axhline(0, color='gray', linewidth=0.5)\n",
    "plt.axvline(0, color='gray', linewidth=0.5)\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "plt.title(f\"Vetores de palavras em 2D — '{target1_text}' (azul) e '{target2_text}' (verde)\")\n",
    "plt.xlabel(\"Dimensão 1\")\n",
    "plt.ylabel(\"Dimensão 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ad1d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# escravo (azul) e senhor (verde)\n",
    "target1_text = \"escravo\"\n",
    "target2_text = \"senhor\"\n",
    "\n",
    "t1 = nlp(target1_text)[0]\n",
    "t2 = nlp(target2_text)[0]\n",
    "\n",
    "if nlp.vocab.vectors_length == 0:\n",
    "    raise RuntimeError(\"O modelo atual não tem vetores (nlp.vocab.vectors_length == 0). \"\n",
    "                       \"Instale um modelo com vetores: python -m spacy download pt_core_news_md (ou lg).\")\n",
    "\n",
    "topn = 10\n",
    "top1 = [w for w,_ in top_n_similar_from_vocab(t1.vector, t1.text, nlp.vocab, n=topn)]\n",
    "top2 = [w for w,_ in top_n_similar_from_vocab(t2.vector, t2.text, nlp.vocab, n=topn)]\n",
    "\n",
    "# criar lista única preservando ordem (primeiro alvo1 e seus top, depois alvo2 e seus top não repetidos)\n",
    "words = [target1_text] + top1 + [w for w in [target2_text] + top2 if w not in ([target1_text] + top1)]\n",
    "\n",
    "# montar matriz de vetores\n",
    "vecs = [nlp.vocab[w].vector for w in words]\n",
    "X = np.vstack(vecs)\n",
    "\n",
    "# projetar em 2D com SVD\n",
    "Xc = X - X.mean(axis=0)\n",
    "U, S, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
    "proj = Xc.dot(Vt[:2].T)\n",
    "\n",
    "# determinar cor por origem\n",
    "colors = []\n",
    "for w in words:\n",
    "    if w == target1_text:\n",
    "        colors.append('tab:blue')\n",
    "    elif w == target2_text:\n",
    "        colors.append('tab:green')\n",
    "    elif w in top1 and w in top2:\n",
    "        colors.append('tab:purple')  # aparece em ambos\n",
    "    elif w in top1:\n",
    "        colors.append('#4da6ff')  # azul claro\n",
    "    elif w in top2:\n",
    "        colors.append('#66cc66')  # verde claro\n",
    "    else:\n",
    "        colors.append('gray')\n",
    "\n",
    "origin = np.array([0.0, 0.0])\n",
    "plt.figure(figsize=(10,6))\n",
    "for i, w in enumerate(words):\n",
    "    x, y = proj[i]\n",
    "    width_head = 0.06 * max(1, np.linalg.norm(proj[i]))\n",
    "    length_head = 0.09 * max(1, np.linalg.norm(proj[i]))\n",
    "    linewidth = 2.2 if w in (target1_text, target2_text) else 1.2\n",
    "    plt.arrow(origin[0], origin[1], x, y,\n",
    "              head_width=width_head, head_length=length_head,\n",
    "              fc=colors[i], ec=colors[i], length_includes_head=True,\n",
    "              alpha=0.9, linewidth=linewidth)\n",
    "    plt.text(x*1.06, y*1.06, w, fontsize=12, ha='center', va='center', color=colors[i])\n",
    "\n",
    "plt.axhline(0, color='gray', linewidth=0.5)\n",
    "plt.axvline(0, color='gray', linewidth=0.5)\n",
    "plt.grid(True, linestyle='--', alpha=0.4)\n",
    "plt.title(f\"Vetores de palavras em 2D — '{target1_text}' (azul) e '{target2_text}' (verde)\")\n",
    "plt.xlabel(\"Dimensão 1\")\n",
    "plt.ylabel(\"Dimensão 2\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
